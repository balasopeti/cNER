{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910b7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075cca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt the user to enter the file path\n",
    "file_path = \"MasterList (3).xlsx\"\n",
    "\n",
    "# Check if the file exists\n",
    "#if not os.path.exists(file_path):\n",
    "   # print(\"File not found!\")\n",
    "   # exit()\n",
    "\n",
    "# Load the data from the Excel sheet\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f621b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the texts and the names\n",
    "texts = df['Text'].tolist()\n",
    "names = df['Name'].tolist()\n",
    "\n",
    "# Create a tokenizer for the texts and the names\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(texts + names)\n",
    "\n",
    "# Convert the texts and the names to sequences of integers\n",
    "text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "name_sequences = tokenizer.texts_to_sequences(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc115b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences with zeros to the maximum sequence length\n",
    "max_length = max([len(seq) for seq in text_sequences + name_sequences])\n",
    "text_sequences_padded = pad_sequences(text_sequences, maxlen=max_length, padding='post')\n",
    "name_sequences_padded = pad_sequences(name_sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60050515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the name sequences to one-hot encoding\n",
    "name_sequences_onehot = to_categorical(name_sequences_padded, num_classes=len(tokenizer.word_index) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f205c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "train_texts, test_texts, train_names, test_names = train_test_split(text_sequences_padded, name_sequences_onehot, test_size=0.2, random_state=42)\n",
    "train_texts, val_texts, train_names, val_names = train_test_split(train_texts, train_names, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beff233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the Keras model\n",
    "def create_model(input_dim, output_dim, max_length):\n",
    "    model = Sequential([\n",
    "        Input(shape=(max_length,)),\n",
    "        Embedding(input_dim=input_dim, output_dim=32, input_length=max_length),\n",
    "        Bidirectional(LSTM(units=32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        LSTM(units=32, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),\n",
    "        TimeDistributed(Dense(output_dim, activation=\"softmax\"))\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76d0e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the input and output dimensions\n",
    "input_dim = len(tokenizer.word_index) + 1\n",
    "output_dim = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Create the Keras model\n",
    "model = create_model(input_dim, output_dim, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a1bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - 304s 77s/step - loss: 3.9752 - accuracy: 0.6888 - val_loss: 3.9154 - val_accuracy: 0.9970\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 423s 117s/step - loss: 3.8561 - accuracy: 0.9968 - val_loss: 3.7253 - val_accuracy: 0.9970\n",
      "Epoch 3/30\n",
      "3/4 [=====================>........] - ETA: 15:14 - loss: 3.6339 - accuracy: 0.9968"
     ]
    }
   ],
   "source": [
    "# Train the model on the training set and validate on the validation set\n",
    "history = model.fit(train_texts, train_names, epochs=30, batch_size=16, validation_data=(val_texts, val_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528cf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_texts, test_names)\n",
    "print(\"Test loss:\", loss)\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract the named entities from a text using the trained model\n",
    "def extract_named_entities(text, model, tokenizer):\n",
    "    # Tokenize the text\n",
    "    text_sequence = tokenizer.texts_to_sequences([text])\n",
    "    text_sequence_padded = pad_sequences(text_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(text_sequence_padded)[0]\n",
    "\n",
    "    # Extract the named entities from the predictions\n",
    "    name_indices = np.argmax(predictions, axis=-1)\n",
    "    name_tokens = [tokenizer.index_word[idx] if idx > 0 else '' for idx in name_indices]\n",
    "\n",
    "    # Create a list of spans representing the named entities\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, token in enumerate(name_tokens):\n",
    "        if token and not start:\n",
    "            start = i\n",
    "        elif not token and start:\n",
    "            end = i\n",
    "            spans.append((start, end))\n",
    "            start = None\n",
    "    if start:\n",
    "        end = len(name_tokens)\n",
    "        spans.append((start, end))\n",
    "\n",
    "    # Create a list of spaCy entities from the spans\n",
    "    entities = []\n",
    "    for start, end in spans:\n",
    "        entities.append(spacy.tokens.Span(doc, start, end, label='PERSON'))\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Define a function to add the named entities to a spaCy doc\n",
    "def add_named_entities(doc, entities):\n",
    "    for ent in entities:\n",
    "        doc.ents += (ent,)\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to add the named entities to a spaCy doc\n",
    "def add_named_entities(doc, entities):\n",
    "    for ent in entities:\n",
    "        doc.ents += (ent,)\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05add856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the named entity recognition component to the pipeline\n",
    "nlp.add_pipe(nlp.create_pipe('ner'), last=True)\n",
    "\n",
    "# Process a text and extract the named entities\n",
    "text = \" Dr. Sharath Chandra Mouli is a medical doctor who specializes in the field of Gastroenterology. He is associated with the Krishna Institute of Medical Sciences (KIMS) Hospital in Secunderabad, Telangana, India.Dr. Mouli completed his MBBS degree from the Rajiv Gandhi University of Health Sciences in Bangalore, India, and then went on to pursue a Doctorate of Medicine (DM) in Gastroenterology from the Nizam's Institute of Medical Sciences in Hyderabad, India. He has several years of experience in the field of Gastroenterology and has worked at various prestigious institutions across India.Dr. Mouli's areas of expertise include the diagnosis and treatment of various gastrointestinal disorders such as inflammatory bowel disease, liver diseases, pancreatic disorders, and motility disorders. He is also trained in performing advanced endoscopic procedures such as endoscopic ultrasound (EUS), endoscopic retrograde cholangiopancreatography (ERCP), and endoscopic mucosal resection (EMR).Apart from his clinical work, Dr. Mouli is also involved in research activities in the field of Gastroenterology and has published several articles in peer-reviewed medical journals.\"\n",
    "doc = nlp(text)\n",
    "entities = extract_named_entities(text, model, tokenizer)\n",
    "doc = add_named_entities(doc, entities)\n",
    "\n",
    "# Print the named entities\n",
    "print(\"Named entities found:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ea140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the named entities using displaCy\n",
    "spacy.displacy.render(doc, style='ent')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
