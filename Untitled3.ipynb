{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "146d62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f67c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auth_name</th>\n",
       "      <th>fn</th>\n",
       "      <th>interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dr. Yue Cao is a highly respected radiologist ...</td>\n",
       "      <td>Yue Cao</td>\n",
       "      <td>Quantitative imaging for tumor and normal tiss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biography of Dr. Bensheng Qiu, Radiologist\\n\\n...</td>\n",
       "      <td>Bensheng Qiu</td>\n",
       "      <td>advancing the field of radiology,developing in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biography of Dr. Robert Fleck, Radiologist\\n\\n...</td>\n",
       "      <td>Robert Fleck J</td>\n",
       "      <td>imaging in early cancer detection, and his wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Holden Wu is a renowned radiologist who ha...</td>\n",
       "      <td>Holden Wu</td>\n",
       "      <td>novel imaging modalities, such as cardiac magn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biography of Dr. William Hyslop: Radiologist E...</td>\n",
       "      <td>William Hyslop</td>\n",
       "      <td>advanced imaging techniques, such as functiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           auth_name              fn  \\\n",
       "0  Dr. Yue Cao is a highly respected radiologist ...         Yue Cao   \n",
       "1  Biography of Dr. Bensheng Qiu, Radiologist\\n\\n...    Bensheng Qiu   \n",
       "2  Biography of Dr. Robert Fleck, Radiologist\\n\\n...  Robert Fleck J   \n",
       "3  Dr. Holden Wu is a renowned radiologist who ha...       Holden Wu   \n",
       "4  Biography of Dr. William Hyslop: Radiologist E...  William Hyslop   \n",
       "\n",
       "                                            interest  \n",
       "0  Quantitative imaging for tumor and normal tiss...  \n",
       "1  advancing the field of radiology,developing in...  \n",
       "2  imaging in early cancer detection, and his wor...  \n",
       "3  novel imaging modalities, such as cardiac magn...  \n",
       "4  advanced imaging techniques, such as functiona...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19eaf4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for index, row in df.iterrows():\n",
    "    print(index)\n",
    "    text = row['auth_name']\n",
    "    name = row['fn']\n",
    "    try:\n",
    "        entities = [{'start': text.index(name), 'end': text.index(name) + len(name), 'label': 'PERSON'}]\n",
    "        data.append((text, {'entities': entities}))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4d24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('spacy_input.json', 'w') as f:\n",
    "    for d in data:\n",
    "        json.dump({'text': d[0], 'annotations': d[1]}, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd1aa825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "with open('spacy_input.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "270eb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the list as a JSON file\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f27c2350",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as f:\n",
    "    data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa3368a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Optimizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20640\\3086150847.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Define your optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     31\u001b[0m                         \u001b[0mmaximize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforeach\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcapturable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                         differentiable=differentiable, fused=fused)\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mparam_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Optimizer' object is not iterable"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Define your custom neural network architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(1), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(2, x.size(1), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Load the blank Spacy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add a named entity recognizer to the pipeline with your custom architecture\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "output_size = ner.add_label(\"PERSON\")\n",
    "\n",
    "# Load the training data in JSON format\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(nlp.create_optimizer())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        # Create the input features from the text\n",
    "        input_feats = [...] # Convert texts to input features\n",
    "        input_tensors = torch.tensor(input_feats).float().transpose(0, 1)\n",
    "        # Create the expected output labels\n",
    "        target_labels = [...] # Convert annotations to expected output labels\n",
    "        target_tensors = torch.tensor(target_labels).long().transpose(0, 1)\n",
    "        # Zero the gradients, run the model, and compute the loss\n",
    "        optimizer.zero_grad()\n",
    "        output = nlp.entity(input_tensors)\n",
    "        loss = nn.functional.cross_entropy(output.reshape(-1, output_size), target_tensors.reshape(-1))\n",
    "        # Backpropagate and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if \"ner\" not in losses:\n",
    "            losses[\"ner\"] = loss.item()\n",
    "        else:\n",
    "            losses[\"ner\"] += loss.item()\n",
    "\n",
    "    print(losses)\n",
    "\n",
    "# Save the trained model\n",
    "nlp.to_disk(\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70638b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
