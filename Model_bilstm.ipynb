{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf449e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports required\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from numpy import array\n",
    "from numpy.random import seed\n",
    "from keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d245dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to import data and return it in a dataframe\n",
    "\n",
    "def getRawData(path):\n",
    "    return pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8894bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define token to id and id to token\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Text'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Label'].to_list()))\n",
    "     # Convert vocab elements to strings\n",
    "    vocab = [str(x) for x in vocab]\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02904ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get extra columns of token to id representation for text and label\n",
    "def getDataTagged(data,tok2idx,tag2idx):\n",
    "    data['Word_idx'] = data['Text'].map(token2idx)\n",
    "    data['Tag_idx'] = data['Label'].map(tag2idx)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe3b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupedData(data):\n",
    "    data_fillna = data.fillna(method='ffill', axis=0)\n",
    "    for i in range(len(data)):\n",
    "        print(i)\n",
    "        data_group = data_fillna.groupby([\"Sentence\"],as_index=False)['Text', 'POS', 'Label', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "    return data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2637e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Text'].to_list())))\n",
    "    n_tag = len(list(set(data['Label'].to_list())))   \n",
    "    tokens = data_group['Word_idx'].to_list()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "    tags = data_group['Tag_idx'].to_list()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post')\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "    print(\n",
    "        'train_tokens length:', len(tokens_),\n",
    "        '\\ntrain_tags length:',len(tags_),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "    )\n",
    "    return tokens_, test_tokens, tags_, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0853b5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a75701",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=getRawData('improved_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8df08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(data.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3425c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=pd.read_excel(\"groupData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc854b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Text</th>\n",
       "      <th>POS</th>\n",
       "      <th>Label</th>\n",
       "      <th>Word_idx</th>\n",
       "      <th>Tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>['Dr.', 'Yue', 'Cao', 'is', 'a', 'highly', 're...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'RB', 'JJ',...</td>\n",
       "      <td>[nan, 'PERSON', 'PERSON', 'PERSON', 'PERSON', ...</td>\n",
       "      <td>[1970, 720, 956, 804, 1180, 244, 405, 854, 576...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence 10</td>\n",
       "      <td>['Biography', 'of', 'Dr.', 'John', 'Kurhanewic...</td>\n",
       "      <td>['NN', 'IN', 'NNP', 'NNP', 'NNP', ',', 'NNP', ...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[105, 979, 1970, 1684, 1425, 1145, 9, 1136, 19...</td>\n",
       "      <td>[0, 0, 0, 12, 12, 0, 4, 0, 0, 12, 12, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence 100</td>\n",
       "      <td>['Dr.', 'David', 'Bluemke', 'is', 'a', 'renown...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 1438, 1802, 804, 1180, 736, 854, 308, 9...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sentence 101</td>\n",
       "      <td>['Dr.', 'David', 'Nascene', 'is', 'a', 'renown...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 1438, 59, 804, 1180, 736, 854, 308, 976...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sentence 11</td>\n",
       "      <td>['Dr.', 'Anderanik', 'Tomasian', 'is', 'a', 'r...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 1019, 377, 804, 1180, 736, 854, 576, 11...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Sentence 95</td>\n",
       "      <td>['Dr.', 'James', 'Babb', 'is', 'a', 'renowned'...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 1042, 1215, 804, 1180, 736, 854, 308, 9...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Sentence 96</td>\n",
       "      <td>['Dr.', 'Mauricio', 'Castillo', 'is', 'a', 're...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 1080, 713, 804, 1180, 736, 854, 576, 11...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Sentence 97</td>\n",
       "      <td>['Dr.', 'Claude', 'Sirlin', 'is', 'a', 'highly...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'RB', 'JJ',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 1118, 1172, 804, 1180, 244, 115, 854, 3...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>Sentence 98</td>\n",
       "      <td>['Dr.', 'Martin', 'Prince', 'is', 'a', 'renown...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 20, 1435, 804, 1180, 736, 854, 932, 131...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>Sentence 99</td>\n",
       "      <td>['Dr.', 'Scott', 'Reeder', 'is', 'a', 'renowne...</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...</td>\n",
       "      <td>[1970, 874, 1967, 804, 1180, 736, 854, 308, 97...</td>\n",
       "      <td>[0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0      Sentence  \\\n",
       "0             0    Sentence 1   \n",
       "1             1   Sentence 10   \n",
       "2             2  Sentence 100   \n",
       "3             3  Sentence 101   \n",
       "4             4   Sentence 11   \n",
       "..          ...           ...   \n",
       "96           96   Sentence 95   \n",
       "97           97   Sentence 96   \n",
       "98           98   Sentence 97   \n",
       "99           99   Sentence 98   \n",
       "100         100   Sentence 99   \n",
       "\n",
       "                                                  Text  \\\n",
       "0    ['Dr.', 'Yue', 'Cao', 'is', 'a', 'highly', 're...   \n",
       "1    ['Biography', 'of', 'Dr.', 'John', 'Kurhanewic...   \n",
       "2    ['Dr.', 'David', 'Bluemke', 'is', 'a', 'renown...   \n",
       "3    ['Dr.', 'David', 'Nascene', 'is', 'a', 'renown...   \n",
       "4    ['Dr.', 'Anderanik', 'Tomasian', 'is', 'a', 'r...   \n",
       "..                                                 ...   \n",
       "96   ['Dr.', 'James', 'Babb', 'is', 'a', 'renowned'...   \n",
       "97   ['Dr.', 'Mauricio', 'Castillo', 'is', 'a', 're...   \n",
       "98   ['Dr.', 'Claude', 'Sirlin', 'is', 'a', 'highly...   \n",
       "99   ['Dr.', 'Martin', 'Prince', 'is', 'a', 'renown...   \n",
       "100  ['Dr.', 'Scott', 'Reeder', 'is', 'a', 'renowne...   \n",
       "\n",
       "                                                   POS  \\\n",
       "0    ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'RB', 'JJ',...   \n",
       "1    ['NN', 'IN', 'NNP', 'NNP', 'NNP', ',', 'NNP', ...   \n",
       "2    ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "3    ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "4    ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "..                                                 ...   \n",
       "96   ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "97   ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "98   ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'RB', 'JJ',...   \n",
       "99   ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "100  ['NNP', 'NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN',...   \n",
       "\n",
       "                                                 Label  \\\n",
       "0    [nan, 'PERSON', 'PERSON', 'PERSON', 'PERSON', ...   \n",
       "1    ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "2    ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "3    ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "4    ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "..                                                 ...   \n",
       "96   ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "97   ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "98   ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "99   ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "100  ['PERSON', 'PERSON', 'PERSON', 'PERSON', 'PERS...   \n",
       "\n",
       "                                              Word_idx  \\\n",
       "0    [1970, 720, 956, 804, 1180, 244, 405, 854, 576...   \n",
       "1    [105, 979, 1970, 1684, 1425, 1145, 9, 1136, 19...   \n",
       "2    [1970, 1438, 1802, 804, 1180, 736, 854, 308, 9...   \n",
       "3    [1970, 1438, 59, 804, 1180, 736, 854, 308, 976...   \n",
       "4    [1970, 1019, 377, 804, 1180, 736, 854, 576, 11...   \n",
       "..                                                 ...   \n",
       "96   [1970, 1042, 1215, 804, 1180, 736, 854, 308, 9...   \n",
       "97   [1970, 1080, 713, 804, 1180, 736, 854, 576, 11...   \n",
       "98   [1970, 1118, 1172, 804, 1180, 244, 115, 854, 3...   \n",
       "99   [1970, 20, 1435, 804, 1180, 736, 854, 932, 131...   \n",
       "100  [1970, 874, 1967, 804, 1180, 736, 854, 308, 97...   \n",
       "\n",
       "                                               Tag_idx  \n",
       "0    [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "1    [0, 0, 0, 12, 12, 0, 4, 0, 0, 12, 12, 0, 0, 0,...  \n",
       "2    [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "3    [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4    [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "..                                                 ...  \n",
       "96   [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "97   [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "98   [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "99   [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "100  [0, 12, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "\n",
       "[101 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06d57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=data_group.drop(data_group.columns[0],axis=1)\n",
    "tokens = data_group['Word_idx'].tolist()\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=list(map(int,tokens[i][1:-1].split(\",\")))\n",
    "data_group['Word_idx']=tokens\n",
    "tags = data_group['Tag_idx'].tolist()\n",
    "for i in range(len(tags)):\n",
    "    tags[i]=list(map(int,tags[i][1:-1].split(\",\")))\n",
    "data_group['Tag_idx']=tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "745bb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c99527e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data_idx=getDataTagged(data,token2idx,tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bfe7d",
   "metadata": {},
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1180b2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan': 0,\n",
       " 'PERSON': 1,\n",
       " 'LOC': 2,\n",
       " 'LAW': 3,\n",
       " 'NORP': 4,\n",
       " 'CARDINAL': 5,\n",
       " 'ORDINAL': 6,\n",
       " 'ORG': 7,\n",
       " 'WORK_OF_ART': 8,\n",
       " 'FAC': 9,\n",
       " 'EVENT': 10,\n",
       " 'PRODUCT': 11,\n",
       " 'GPE': 12,\n",
       " 'DATE': 13}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caaf5733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 90 \n",
      "train_tags length: 90 \n",
      "test_tokens length: 11 \n",
      "test_tags: 11\n"
     ]
    }
   ],
   "source": [
    "train_tokens, test_tokens, train_tags,test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1ad39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiru\\AppData\\Local\\Temp\\ipykernel_4324\\3717584689.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  train_names = torch.tensor(train_tags, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_texts = torch.tensor(train_tokens, dtype=torch.long)\n",
    "train_names = torch.tensor(train_tags, dtype=torch.float)\n",
    "val_texts = torch.tensor(test_tokens, dtype=torch.long)\n",
    "val_names = torch.tensor(test_tags, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f4e202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d586b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_names)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_texts, val_names)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a5b95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, input_length, n_tags):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, output_dim)\n",
    "        self.bidirectional_lstm = nn.LSTM(output_dim, output_dim, bidirectional=True, dropout=0.2, batch_first=True)\n",
    "        self.lstm = nn.LSTM(2*output_dim, output_dim, dropout=0.5, batch_first=True)\n",
    "        self.time_distributed = nn.Linear(output_dim, n_tags)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bidirectional_lstm(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.time_distributed(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce02066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim= 2033\n",
      "output_dim= 64\n",
      "input_length= 713\n",
      "number of unique tags= 14\n",
      "2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiru\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\thiru\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(data['Text'].to_list())))+1 # Your vocabulary size\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()]) # The maximum length of your input sequences\n",
    "n_tags =  len(tag2idx) # The number of unique tags in your NER dataset\n",
    "model = NERModel(input_dim, output_dim, input_length, n_tags)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "print('input_dim=',input_dim)\n",
    "print('output_dim=',output_dim)\n",
    "print('input_length=',input_length)\n",
    "print('number of unique tags=',n_tags)\n",
    "\n",
    "print(len(set(data['Text'].to_list())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c825c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 2.636612137158712, Val Loss: 2.628167152404785\n",
      "Epoch 2/30, Train Loss: 2.6215038696924844, Val Loss: 2.6056041717529297\n",
      "Epoch 3/30, Train Loss: 2.596214691797892, Val Loss: 2.5611095428466797\n",
      "Epoch 4/30, Train Loss: 2.545430580774943, Val Loss: 2.480551242828369\n",
      "Epoch 5/30, Train Loss: 2.4374778270721436, Val Loss: 2.3076555728912354\n",
      "Epoch 6/30, Train Loss: 2.2041214307149253, Val Loss: 2.046205997467041\n",
      "Epoch 7/30, Train Loss: 1.9858741164207458, Val Loss: 1.9238417148590088\n",
      "Epoch 8/30, Train Loss: 1.9095744291941326, Val Loss: 1.8902039527893066\n",
      "Epoch 9/30, Train Loss: 1.888508677482605, Val Loss: 1.8791836500167847\n",
      "Epoch 10/30, Train Loss: 1.8787593841552734, Val Loss: 1.8745659589767456\n",
      "Epoch 11/30, Train Loss: 1.8768525123596191, Val Loss: 1.8723028898239136\n",
      "Epoch 12/30, Train Loss: 1.874928851922353, Val Loss: 1.8710343837738037\n",
      "Epoch 13/30, Train Loss: 1.872561017672221, Val Loss: 1.8702316284179688\n",
      "Epoch 14/30, Train Loss: 1.8713339567184448, Val Loss: 1.869670033454895\n",
      "Epoch 15/30, Train Loss: 1.8724415103594463, Val Loss: 1.8692442178726196\n",
      "Epoch 16/30, Train Loss: 1.870399792989095, Val Loss: 1.8689028024673462\n",
      "Epoch 17/30, Train Loss: 1.871297041575114, Val Loss: 1.8686162233352661\n",
      "Epoch 18/30, Train Loss: 1.8710374633471172, Val Loss: 1.8683686256408691\n",
      "Epoch 19/30, Train Loss: 1.8706727027893066, Val Loss: 1.868149995803833\n",
      "Epoch 20/30, Train Loss: 1.8721855680147808, Val Loss: 1.8679544925689697\n",
      "Epoch 21/30, Train Loss: 1.8713891903559368, Val Loss: 1.8677778244018555\n",
      "Epoch 22/30, Train Loss: 1.870774229367574, Val Loss: 1.8676166534423828\n",
      "Epoch 23/30, Train Loss: 1.8710662126541138, Val Loss: 1.8674689531326294\n",
      "Epoch 24/30, Train Loss: 1.8710289398829143, Val Loss: 1.867333173751831\n",
      "Epoch 25/30, Train Loss: 1.8716155489285786, Val Loss: 1.867207407951355\n",
      "Epoch 26/30, Train Loss: 1.8704481720924377, Val Loss: 1.867091178894043\n",
      "Epoch 27/30, Train Loss: 1.8699166973431904, Val Loss: 1.8669829368591309\n",
      "Epoch 28/30, Train Loss: 1.8699959715207417, Val Loss: 1.8668822050094604\n",
      "Epoch 29/30, Train Loss: 1.8688232898712158, Val Loss: 1.8667880296707153\n",
      "Epoch 30/30, Train Loss: 1.8697986404101055, Val Loss: 1.8666998147964478\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Reshape labels for the loss function\n",
    "        labels = labels.view(-1, n_tags)  # Modify this line\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(val_loader):\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Reshape labels for the loss function\n",
    "            labels = labels.view(-1, n_tags)  # Modify this line\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc3aae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_named_entities(text):\n",
    "    # Tokenize the input text using spacy\n",
    "    tokenized_text = nlp(text)\n",
    "    \n",
    "    # Extract tokens as strings\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "\n",
    "    # Convert tokens to indices using token2idx dictionary\n",
    "    #input_tokens = [token2idx.get(token, token2idx['<unknown>']) for token in tokens] # Replace '<unknown>' with the correct unknown token representation in your token2idx dictionary\n",
    "    input_tokens = [token2idx.get(token, len(token2idx) - 1) for token in tokens]\n",
    "    \n",
    "    # Convert the input tokens to PyTorch tensor\n",
    "    input_tensor = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)\n",
    "    \n",
    "    # Run the model on the input tensor\n",
    "    outputs = model(input_tensor)\n",
    "    \n",
    "    # Get the index of the maximum value for each token\n",
    "    predictions = torch.argmax(outputs, dim=-1)\n",
    "    \n",
    "    return predictions.squeeze(dim=0), tokens\n",
    "\n",
    "def post_process(predictions, tokens):\n",
    "    entities = []\n",
    "    labels = [idx2tag[pred.item()] for pred in predictions]\n",
    "    for i, token_label in enumerate(labels):\n",
    "        if token_label.startswith(\"B\") or token_label.startswith(\"I\"):\n",
    "            entities.append((tokens[i], token_label))\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19e1fa43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = \"John Doe works at Google in Mountain View, California.\"\n",
    "predictions, tokens = predict_named_entities(text)\n",
    "entities = post_process(predictions, tokens)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050b894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f03ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5debd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f43d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a02213",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E047] Can't assign a value to unregistered extension attribute 'predicted_entity'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4324\\3794151129.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Dr. Yue Cao is a highly respected radiologist with a wealth of experience in the field of medical imaging. Born on July 12, 1975, in Shanghai, China, Dr. Cao showed an early aptitude for science and a deep curiosity about the human body, which eventually led him to pursue a career in medicine. Dr. Cao completed his undergraduate studies in Medicine at Fudan University, one of the most prestigious universities in China. He then obtained his medical degree from Shanghai Medical College of Fudan University, where he graduated with top honors. He went on to complete his residency in Radiology at Huashan Hospital, also affiliated with Fudan University, where he developed a keen interest in diagnostic and interventional radiology.   After completing his residency, Dr. Cao embarked on a successful career in radiology, working at several renowned hospitals in China. He later moved to the United States to further his education and professional development. He completed a Fellowship in Radiology at Harvard Medical School and Massachusetts General Hospital, where he gained expertise in advanced imaging techniques and interventional radiology procedures. Dr. Cao's medical career has spanned over two decades, during which he has held various positions at leading medical institutions. Currently, he serves as the Chief of Radiology at a prominent hospital in New York City, where he leads a team of radiologists and oversees the diagnostic imaging services. Dr. Cao is known for his strong interest in medical research and has made significant contributions to the field of radiology. He has published numerous research papers in renowned medical journals and has been invited to speak at national and international conferences on topics such as advanced imaging techniques, minimally invasive procedures, and emerging technologies in radiology. His special interests include oncologic imaging, cardiovascular imaging, and image-guided interventions. Dr. Cao has also been involved in clinical trials and collaborative research projects with other medical institutions, aiming to advance the field of radiology and improve patient care. Dr. Cao has authored several books on radiology, which are widely recognized as valuable resources for medical professionals. His books cover various aspects of radiology, including diagnostic imaging, interventional radiology, and emerging trends in the field. His publications are known for their comprehensive and practical approach, providing insights into the latest advancements in radiology. Dr. Cao's contributions to the field of radiology have been recognized with numerous awards and honors. He has received prestigious awards from professional societies for his outstanding contributions to the field of radiology, including the American College of Radiology (ACR) and the Radiological Society of North America (RSNA). He is also an active member of several professional organizations, including the Society of Interventional Radiology (SIR) and the American Roentgen Ray Society (ARRS). In addition to his professional achievements, Dr. Cao is known for his compassionate and patient-centric approach to healthcare. He is committed to providing the highest quality of care to his patients and is dedicated to advancing the field of radiology through his research, clinical practice, and teaching. Dr. Cao's passion for radiology, his dedication to patient care, and his significant contributions to the field make him a respected and renowned figure in the medical community. His expertise and commitment to excellence continue to impact the field of radiology, improving patient outcomes and advancing the practice of medical imaging. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4324\\3794151129.py\u001b[0m in \u001b[0;36mner_pipeline\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mdisplacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ent\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"ents\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"colors\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"PER\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"lightblue\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LOC\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"yellow\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ORG\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"purple\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"INTEREST\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"orange\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4324\\3794151129.py\u001b[0m in \u001b[0;36mpost_process\u001b[1;34m(output, tokens)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_tag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_entity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\underscore.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE047\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msetter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E047] Can't assign a value to unregistered extension attribute 'predicted_entity'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "# Register the extension\n",
    "#spacy.tokens.Token.set_extension(\"predicted_entity\", default=None)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.text for token in doc]\n",
    "    return tokens\n",
    "\n",
    "# Preparing input tensor function\n",
    "def prepare_input(tokens):\n",
    "    token_indices = [token2idx.get(token, len(token2idx) - 1) for token in tokens]\n",
    "    maxlen = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "    pad_tokens = pad_sequences([token_indices], maxlen=maxlen, dtype='int32', padding='post', value=len(token2idx) - 1)\n",
    "    input_tensor = torch.tensor(pad_tokens, dtype=torch.long)\n",
    "    return input_tensor\n",
    "\n",
    "# Prediction function\n",
    "def predict_entities(input_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    return output\n",
    "#Token.set_extension(\"predicted_entity\", default=\"\")\n",
    "\n",
    "# Post-processing function\n",
    "def post_process(output, tokens):\n",
    "    _, predicted_indices = torch.max(output, dim=-1)\n",
    "    predicted_indices = predicted_indices.squeeze(0).numpy()\n",
    "    predicted_tags = [idx2tag[index] for index in predicted_indices]\n",
    "   \n",
    "    # Replace 'nan' tags with 'INTEREST'\n",
    "   # predicted_tags = ['INTEREST' if tag == 'nan' else tag for tag in predicted_tags]\n",
    "\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    for token, pred_tag in zip(doc, predicted_tags):\n",
    "        token._.predicted_entity = pred_tag\n",
    "\n",
    "    return doc\n",
    "    #result = list(zip(tokens, predicted_tags))\n",
    "    #return result\n",
    "\n",
    "# NER pipeline function\n",
    "def ner_pipeline(text):\n",
    "    tokens = preprocess_text(text)\n",
    "    input_tensor = prepare_input(tokens)\n",
    "    output = predict_entities(input_tensor)\n",
    "    doc = post_process(output, tokens)\n",
    "    displacy.render(doc, style=\"ent\", options={\"ents\": list(tag2idx.keys()) , \"colors\": {\"PER\": \"lightblue\", \"LOC\": \"yellow\", \"ORG\": \"purple\", \"INTEREST\": \"orange\"}})\n",
    "\n",
    "    return doc\n",
    "   \n",
    "   \n",
    "    #return result\n",
    "\n",
    "# Example usage\n",
    "text = \"Dr. Yue Cao is a highly respected radiologist with a wealth of experience in the field of medical imaging. Born on July 12, 1975, in Shanghai, China, Dr. Cao showed an early aptitude for science and a deep curiosity about the human body, which eventually led him to pursue a career in medicine. Dr. Cao completed his undergraduate studies in Medicine at Fudan University, one of the most prestigious universities in China. He then obtained his medical degree from Shanghai Medical College of Fudan University, where he graduated with top honors. He went on to complete his residency in Radiology at Huashan Hospital, also affiliated with Fudan University, where he developed a keen interest in diagnostic and interventional radiology.   After completing his residency, Dr. Cao embarked on a successful career in radiology, working at several renowned hospitals in China. He later moved to the United States to further his education and professional development. He completed a Fellowship in Radiology at Harvard Medical School and Massachusetts General Hospital, where he gained expertise in advanced imaging techniques and interventional radiology procedures. Dr. Cao's medical career has spanned over two decades, during which he has held various positions at leading medical institutions. Currently, he serves as the Chief of Radiology at a prominent hospital in New York City, where he leads a team of radiologists and oversees the diagnostic imaging services. Dr. Cao is known for his strong interest in medical research and has made significant contributions to the field of radiology. He has published numerous research papers in renowned medical journals and has been invited to speak at national and international conferences on topics such as advanced imaging techniques, minimally invasive procedures, and emerging technologies in radiology. His special interests include oncologic imaging, cardiovascular imaging, and image-guided interventions. Dr. Cao has also been involved in clinical trials and collaborative research projects with other medical institutions, aiming to advance the field of radiology and improve patient care. Dr. Cao has authored several books on radiology, which are widely recognized as valuable resources for medical professionals. His books cover various aspects of radiology, including diagnostic imaging, interventional radiology, and emerging trends in the field. His publications are known for their comprehensive and practical approach, providing insights into the latest advancements in radiology. Dr. Cao's contributions to the field of radiology have been recognized with numerous awards and honors. He has received prestigious awards from professional societies for his outstanding contributions to the field of radiology, including the American College of Radiology (ACR) and the Radiological Society of North America (RSNA). He is also an active member of several professional organizations, including the Society of Interventional Radiology (SIR) and the American Roentgen Ray Society (ARRS). In addition to his professional achievements, Dr. Cao is known for his compassionate and patient-centric approach to healthcare. He is committed to providing the highest quality of care to his patients and is dedicated to advancing the field of radiology through his research, clinical practice, and teaching. Dr. Cao's passion for radiology, his dedication to patient care, and his significant contributions to the field make him a respected and renowned figure in the medical community. His expertise and commitment to excellence continue to impact the field of radiology, improving patient outcomes and advancing the practice of medical imaging. \"\n",
    "doc = ner_pipeline(text)\n",
    "print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6682bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea61bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(result, style='ent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a278b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
