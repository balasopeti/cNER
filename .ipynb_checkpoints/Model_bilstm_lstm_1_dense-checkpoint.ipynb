{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e66f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports required\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from numpy import array\n",
    "from numpy.random import seed\n",
    "from keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f653f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to import data and return it in a dataframe\n",
    "\n",
    "def getRawData(path):\n",
    "    return pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2b546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define token to id and id to token\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Text'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Label'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101e5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get extra columns of token to id representation for text and label\n",
    "def getDataTagged(data,tok2idx,tag2idx):\n",
    "    data['Word_idx'] = data['Text'].map(token2idx)\n",
    "    data['Tag_idx'] = data['Label'].map(tag2idx)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5177dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupedData(data):\n",
    "    data_fillna = data.fillna(method='ffill', axis=0)\n",
    "    for i in range(len(data)):\n",
    "        print(i)\n",
    "        data_group = data_fillna.groupby([\"Sentence\"],as_index=False)['Text', 'POS', 'Label', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "    return data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b88e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Text'].to_list())))\n",
    "    n_tag = len(list(set(data['Label'].to_list())))   \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post')\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "    print(\n",
    "        'train_tokens length:', len(tokens_),\n",
    "        '\\ntrain_tags length:',len(tags_),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "    )\n",
    "    return tokens_, test_tokens, tags_, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ef0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4339273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c095961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Text</th>\n",
       "      <th>POS</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>Dr.</td>\n",
       "      <td>NNP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>Yue</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>Cao</td>\n",
       "      <td>NNP</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sentence 1</td>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58790</th>\n",
       "      <td>601</td>\n",
       "      <td>Sentence 101</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58791</th>\n",
       "      <td>602</td>\n",
       "      <td>Sentence 101</td>\n",
       "      <td>medical</td>\n",
       "      <td>JJ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58792</th>\n",
       "      <td>603</td>\n",
       "      <td>Sentence 101</td>\n",
       "      <td>imaging</td>\n",
       "      <td>NN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58793</th>\n",
       "      <td>604</td>\n",
       "      <td>Sentence 101</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58794</th>\n",
       "      <td>605</td>\n",
       "      <td>Sentence 101</td>\n",
       "      <td>\\n\\n\\n\\n</td>\n",
       "      <td>_SP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58795 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0      Sentence      Text  POS   Label\n",
       "0               0    Sentence 1       Dr.  NNP     NaN\n",
       "1               1    Sentence 1       Yue  NNP  PERSON\n",
       "2               2    Sentence 1       Cao  NNP  PERSON\n",
       "3               3    Sentence 1        is  VBZ     NaN\n",
       "4               4    Sentence 1         a   DT     NaN\n",
       "...           ...           ...       ...  ...     ...\n",
       "58790         601  Sentence 101        of   IN     NaN\n",
       "58791         602  Sentence 101   medical   JJ     NaN\n",
       "58792         603  Sentence 101   imaging   NN     NaN\n",
       "58793         604  Sentence 101         .    .     NaN\n",
       "58794         605  Sentence 101  \\n\\n\\n\\n  _SP     NaN\n",
       "\n",
       "[58795 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=getRawData('improved_data.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19da091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_group=pd.read_excel(\"groupData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29446645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data=getRawData('improved_data.xlsx')\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data_idx=getDataTagged(data,token2idx,tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67291548",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=data_group.drop(data_group.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc30277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)\n",
    "input_dim = len(list(set(data['Text'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77481eb2",
   "metadata": {},
   "source": [
    "data_group=getGroupedData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50388180",
   "metadata": {},
   "source": [
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "#plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)\n",
    "save_model=saveModel(model_bilstm_lstm,save_model)\n",
    "output=save_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_bilstm_lstm_model()\n",
    "print(train_tokens)\n",
    "loss = train_model(train_tokens,train_tags,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tokens[0][3],train_tags[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fa158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ad120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8afff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d22870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=getRawData('improved_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd8676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(data.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61039d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=pd.read_excel(\"groupData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd311fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=data_group.drop(data_group.columns[0],axis=1)\n",
    "tokens = data_group['Word_idx'].tolist()\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=list(map(int,tokens[i][1:-1].split(\",\")))\n",
    "data_group['Word_idx']=tokens\n",
    "tags = data_group['Tag_idx'].tolist()\n",
    "for i in range(len(tags)):\n",
    "    tags[i]=list(map(int,tags[i][1:-1].split(\",\")))\n",
    "data_group['Tag_idx']=tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9170286",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data_idx=getDataTagged(data,token2idx,tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bc55c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 90 \n",
      "train_tags length: 90 \n",
      "test_tokens length: 11 \n",
      "test_tags: 11\n"
     ]
    }
   ],
   "source": [
    "train_tokens, test_tokens, train_tags,test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27eb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c6dce7",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, max_length, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, 32, max_norm=max_length)\n",
    "        self.bi_lstm = nn.LSTM(32, 32, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.lstm = nn.LSTM(64, 32, batch_first=True, dropout=0.5)\n",
    "        self.time_distributed = nn.Linear(32, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.time_distributed(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c953a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiru\\AppData\\Local\\Temp\\ipykernel_26372\\4071512146.py:4: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  train_names = torch.tensor(train_tags, dtype=torch.long)\n",
      "C:\\Users\\thiru\\AppData\\Local\\Temp\\ipykernel_26372\\4071512146.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  train_names = torch.tensor(train_tags, dtype=torch.long)\n",
      "C:\\Users\\thiru\\AppData\\Local\\Temp\\ipykernel_26372\\4071512146.py:6: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  val_names = torch.tensor(test_tags, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_texts = torch.tensor(train_tokens, dtype=torch.long)\n",
    "train_names = torch.tensor(train_tags, dtype=torch.long)\n",
    "val_texts = torch.tensor(test_tokens, dtype=torch.long)\n",
    "val_names = torch.tensor(test_tags, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7212aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "107dee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_names)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_texts, val_names)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfc2b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, input_length, n_tags):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, output_dim)\n",
    "        self.bidirectional_lstm = nn.LSTM(output_dim, output_dim, bidirectional=True, dropout=0.2, batch_first=True)\n",
    "        self.lstm = nn.LSTM(2*output_dim, output_dim, dropout=0.5, batch_first=True)\n",
    "        self.time_distributed = nn.Linear(output_dim, n_tags)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bidirectional_lstm(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.time_distributed(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aedd4b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim= 2033\n",
      "output_dim= 64\n",
      "input_length= 713\n",
      "number of unique tags= 14\n",
      "2032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiru\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\thiru\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(data['Text'].to_list())))+1 # Your vocabulary size\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()]) # The maximum length of your input sequences\n",
    "n_tags =  len(tag2idx) # The number of unique tags in your NER dataset\n",
    "model = NERModel(input_dim, output_dim, input_length, n_tags)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "print('input_dim=',input_dim)\n",
    "print('output_dim=',output_dim)\n",
    "print('input_length=',input_length)\n",
    "print('number of unique tags=',n_tags)\n",
    "\n",
    "print(len(set(data['Text'].to_list())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de9ad3b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (11408) to match target batch_size (159712).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26372\\2170316441.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3026\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (11408) to match target batch_size (159712)."
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 10\n",
    "\n",
    "# Assuming you have already created train_loader and val_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Reshape labels for the loss function\n",
    "        labels = labels.view(-1)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(val_loader):\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Reshape labels for the loss function\n",
    "            labels = labels.view(-1)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bb02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856a6e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4df3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad380ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec928d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b61d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400de497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load your preprocessed data (words and tags)\n",
    "data = train_tokens,train_tags\n",
    "\n",
    "# Function to convert your data into SpaCy's format\n",
    "def convert_data_to_spacy_format(data):\n",
    "    spacy_data = []\n",
    "    for text, tags in data:\n",
    "        entities = []\n",
    "        for tag in tags:\n",
    "            entities.append((tag[0], tag[1], tag[2]))\n",
    "        spacy_data.append((text, {\"entities\": entities}))\n",
    "    return spacy_data\n",
    "\n",
    "# Convert your data to SpaCy format\n",
    "spacy_data = convert_data_to_spacy_format(data)\n",
    "\n",
    "# Create a blank SpaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add custom NER labels to the model\n",
    "for _, annotations in spacy_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Train the custom NER model\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(10): # Number of epochs\n",
    "    random.shuffle(spacy_data)\n",
    "    losses = {}\n",
    "    \n",
    "    batches = minibatch(spacy_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), entities) for text, entities in batch]\n",
    "        nlp.update(examples, drop=0.1, sgd=optimizer, losses=losses)\n",
    "\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(\"custom_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce97038",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = len(list(set(data['Text'].to_list())))+1 # Set the input dimension\n",
    "max_length = max([len(s) for s in data_group['Word_idx'].tolist()]) # Set the maximum sequence length\n",
    "output_dim = 32 # Set the output dimension\n",
    "model = MyModel(input_dim, max_length, output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
