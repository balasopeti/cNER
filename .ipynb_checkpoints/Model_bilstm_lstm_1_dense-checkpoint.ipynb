{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e66f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports required\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from numpy import array\n",
    "from numpy.random import seed\n",
    "from keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f653f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to import data and return it in a dataframe\n",
    "\n",
    "def getRawData(path):\n",
    "    return pd.read_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2b546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define token to id and id to token\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Text'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Label'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101e5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get extra columns of token to id representation for text and label\n",
    "def getDataTagged(data,tok2idx,tag2idx):\n",
    "    data['Word_idx'] = data['Text'].map(token2idx)\n",
    "    data['Tag_idx'] = data['Label'].map(tag2idx)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5177dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroupedData(data):\n",
    "    data_fillna = data.fillna(method='ffill', axis=0)\n",
    "    for i in range(len(data)):\n",
    "        print(i)\n",
    "        data_group = data_fillna.groupby([\"Sentence\"],as_index=False)['Text', 'POS', 'Label', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))\n",
    "    return data_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b88e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "    n_token = len(list(set(data['Text'].to_list())))\n",
    "    n_tag = len(list(set(data['Label'].to_list())))   \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post')\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "    print(\n",
    "        'train_tokens length:', len(tokens_),\n",
    "        '\\ntrain_tags length:',len(tags_),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "    )\n",
    "    return tokens_, test_tokens, tags_, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ef0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4339273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c095961",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=getRawData('improved_data.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19da091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=pd.read_excel(\"groupData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29446645",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data=getRawData('improved_data.xlsx')\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data_idx=getDataTagged(data,token2idx,tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67291548",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=data_group.drop(data_group.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc30277",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)\n",
    "input_dim = len(list(set(data['Text'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77481eb2",
   "metadata": {},
   "source": [
    "data_group=getGroupedData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50388180",
   "metadata": {},
   "source": [
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "#plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)\n",
    "save_model=saveModel(model_bilstm_lstm,save_model)\n",
    "output=save_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a40731d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fa158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ad120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8afff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d22870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=getRawData('improved_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd8676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(data.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61039d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=pd.read_excel(\"groupData.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd311fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group=data_group.drop(data_group.columns[0],axis=1)\n",
    "tokens = data_group['Word_idx'].tolist()\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i]=list(map(int,tokens[i][1:-1].split(\",\")))\n",
    "data_group['Word_idx']=tokens\n",
    "tags = data_group['Tag_idx'].tolist()\n",
    "for i in range(len(tags)):\n",
    "    tags[i]=list(map(int,tags[i][1:-1].split(\",\")))\n",
    "data_group['Tag_idx']=tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9170286",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')\n",
    "data_idx=getDataTagged(data,token2idx,tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80bc55c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 90 \n",
      "train_tags length: 90 \n",
      "test_tokens length: 11 \n",
      "test_tags: 11\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_tags, test_tokens,test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d96402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SlideSciences\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\SlideSciences\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, max_length, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, 32, max_norm=max_length)\n",
    "        self.bi_lstm = nn.LSTM(32, 32, batch_first=True, bidirectional=True, dropout=0.2)\n",
    "        self.lstm = nn.LSTM(64, 32, batch_first=True, dropout=0.5)\n",
    "        self.time_distributed = nn.Linear(32, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bi_lstm(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.time_distributed(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = len(list(set(data['Text'].to_list())))+1 # Set the input dimension\n",
    "max_length = max([len(s) for s in data_group['Word_idx'].tolist()]) # Set the maximum sequence length\n",
    "output_dim = 32 # Set the output dimension\n",
    "model = MyModel(input_dim, max_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c953a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SlideSciences\\AppData\\Local\\Temp\\ipykernel_9060\\4071512146.py:5: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  val_texts = torch.tensor(test_tokens, dtype=torch.long)\n",
      "C:\\Users\\SlideSciences\\AppData\\Local\\Temp\\ipykernel_9060\\4071512146.py:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  val_texts = torch.tensor(test_tokens, dtype=torch.long)\n",
      "C:\\Users\\SlideSciences\\AppData\\Local\\Temp\\ipykernel_9060\\4071512146.py:6: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  val_names = torch.tensor(test_tags, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_texts = torch.tensor(train_tokens, dtype=torch.long)\n",
    "train_names = torch.tensor(train_tags, dtype=torch.long)\n",
    "val_texts = torch.tensor(test_tokens, dtype=torch.long)\n",
    "val_names = torch.tensor(test_tags, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7212aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "107dee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = CustomDataset(train_texts, train_names)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_texts, val_names)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc2b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, input_length, n_tags):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, output_dim)\n",
    "        self.bidirectional_lstm = nn.LSTM(output_dim, output_dim, bidirectional=True, dropout=0.2, batch_first=True)\n",
    "        self.lstm = nn.LSTM(2 * output_dim, output_dim, dropout=0.5, batch_first=True)\n",
    "        self.time_distributed = nn.Linear(output_dim, n_tags)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bidirectional_lstm(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.time_distributed(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aedd4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(data['Text'].to_list())))+1 # Your vocabulary size\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()]) # The maximum length of your input sequences\n",
    "n_tags =  len(tag2idx) # The number of unique tags in your NER dataset\n",
    "model = NERModel(input_dim, output_dim, input_length, n_tags)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "927d8bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1970, 720, 956, 804, 1180, 244, 405, 854, 576, 1180, 323, 979, 1032, 1105, 1875, 450, 979, 72, 1735, 683, 1699, 1300, 225, 1993, 1145, 1639, 1145, 1105, 1539, 1145, 128, 1145, 1970, 956, 1546, 1347, 1648, 27, 976, 0, 1878, 1180, 540, 565, 1799, 1875, 940, 1673, 1145, 664, 38, 430, 1051, 238, 181, 1180, 1235, 1105, 705, 683, 1136, 1709, 1614, 1652, 1371, 1970, 956, 730, 103, 1246, 110, 1105, 1034, 1636, 1946, 721, 1145, 645, 979, 1875, 1244, 564, 684, 1105, 128, 683, 1606, 402, 879, 103, 72, 1517, 134, 1539, 1449, 151, 979, 1946, 721, 1145, 333, 1608, 132, 576, 1585, 1722, 683, 1606, 28, 1300, 238, 1320, 103, 561, 1105, 1185, 1636, 610, 403, 1145, 189, 184, 576, 1946, 721, 1145, 333, 1608, 84, 1180, 1301, 1818, 1105, 1813, 1878, 1218, 751, 683, 1136, 1142, 1614, 1652, 1371, 268, 1925, 103, 561, 1145, 1970, 956, 831, 1300, 1180, 1753, 1235, 1105, 751, 1145, 679, 1636, 249, 736, 800, 1105, 128, 683, 1606, 324, 913, 238, 1875, 1439, 701, 238, 1116, 103, 991, 1878, 943, 1561, 683, 1606, 730, 1180, 2018, 1105, 1185, 1636, 2004, 1449, 1465, 1878, 1026, 1287, 403, 1145, 333, 1608, 1339, 592, 1105, 361, 1735, 1686, 1878, 1218, 751, 702, 683, 1136, 1970, 956, 470, 72, 1235, 1313, 1588, 1251, 1062, 639, 1145, 1231, 664, 1608, 1313, 1279, 242, 1457, 1636, 1159, 72, 1315, 683, 1685, 1145, 1608, 99, 574, 1875, 408, 979, 1185, 1636, 1180, 655, 1052, 1105, 1508, 1102, 1083, 1145, 333, 1608, 1204, 1180, 260, 979, 270, 1878, 968, 1875, 1813, 1735, 663, 683, 1136, 1449, 1774, 1878, 1140, 742, 1652, 1371, 1970, 956, 804, 308, 976, 103, 849, 1818, 1105, 72, 262, 1878, 1313, 325, 954, 305, 238, 1875, 450, 979, 751, 683, 1606, 1313, 1299, 73, 262, 1375, 1105, 736, 72, 690, 1878, 1313, 1641, 312, 238, 1487, 1636, 1560, 1878, 1000, 518, 1300, 353, 197, 574, 361, 1735, 1686, 1145, 638, 1776, 702, 1145, 1878, 255, 1569, 1105, 751, 683, 1597, 1126, 755, 1968, 1942, 1735, 1145, 644, 1735, 1145, 1878, 152, 1165, 1752, 78, 683, 1970, 956, 1313, 189, 1641, 1479, 1105, 1010, 1540, 1878, 321, 262, 91, 576, 1089, 72, 1315, 1145, 365, 238, 904, 1875, 450, 979, 751, 1878, 479, 1990, 342, 683, 1136, 1738, 1652, 1371, 1970, 956, 1313, 1340, 249, 1354, 1300, 751, 1145, 664, 1747, 1877, 727, 574, 158, 1679, 976, 72, 528, 683, 1597, 1354, 772, 242, 1044, 979, 751, 1145, 435, 1813, 1735, 1145, 1218, 751, 1145, 1878, 255, 465, 1105, 1875, 450, 683, 1597, 596, 1747, 308, 976, 369, 39, 1878, 466, 1633, 1145, 17, 1389, 1595, 1875, 542, 1838, 1105, 751, 683, 1136, 70, 1878, 1474, 1652, 1371, 1970, 956, 470, 305, 238, 1875, 450, 979, 751, 578, 1641, 727, 576, 73, 1903, 1878, 1722, 683, 1606, 1313, 445, 564, 1903, 134, 943, 111, 976, 103, 966, 305, 238, 1875, 450, 979, 751, 1145, 435, 1875, 732, 151, 979, 1185, 1918, 180, 1230, 1878, 1875, 895, 801, 979, 1538, 1029, 1918, 1028, 1230, 683, 1606, 804, 189, 1347, 1031, 821, 979, 249, 943, 807, 1145, 435, 1875, 801, 979, 198, 1185, 1918, 337, 1230, 1878, 1875, 732, 1290, 1123, 801, 1918, 1839, 1230, 683, 1136, 443, 1564, 238, 103, 943, 293, 1145, 1970, 956, 804, 308, 976, 103, 1908, 1878, 1990, 1165, 1232, 1633, 238, 806, 683, 1606, 804, 1862, 238, 17, 1875, 120, 1239, 979, 342, 238, 103, 588, 1878, 804, 1103, 238, 155, 1875, 450, 979, 751, 1913, 103, 262, 1145, 1010, 1478, 1145, 1878, 1664, 683, 1136, 1970, 956, 470, 544, 976, 751, 1145, 103, 1594, 238, 1990, 342, 1145, 1878, 103, 954, 305, 238, 1875, 450, 1109, 1051, 1180, 405, 1878, 736, 69, 1105, 1875, 72, 1651, 683, 1597, 592, 1878, 1237, 238, 1059, 154, 238, 1278, 1875, 450, 979, 751, 1145, 35, 1990, 328, 1878, 155, 1875, 1478, 979, 72, 1735, 683]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group['Word_idx'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de9ad3b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 59 is out of bounds for dimension 0 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9060\\3599814891.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9060\\490987258.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 59 is out of bounds for dimension 0 with size 11"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "# Assuming you have already created train_loader and val_loader\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        # Reshape labels for the loss function\n",
    "        labels = labels.view(-1)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(val_loader):\n",
    "            outputs = model(data)\n",
    "\n",
    "            # Reshape labels for the loss function\n",
    "            labels = labels.view(-1)\n",
    "            outputs = outputs.view(-1, outputs.shape[-1])\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7bb02f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
